{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "from RL_brain import PPO\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() \\\n",
    "                            else torch.device('cpu')\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 参数设置\n",
    "# ----------------------------------------- #\n",
    "\n",
    "num_episodes = 100  # 总迭代次数\n",
    "gamma = 0.9  # 折扣因子\n",
    "actor_lr = 1e-3  # 策略网络的学习率\n",
    "critic_lr = 1e-2  # 价值网络的学习率\n",
    "n_hiddens = 16  # 隐含层神经元个数\n",
    "env_name = 'CartPole-v1'\n",
    "return_list = []  # 保存每个回合的return\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 环境加载\n",
    "# ----------------------------------------- #\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "n_states = env.observation_space.shape[0]  # 状态数 4\n",
    "n_actions = env.action_space.n  # 动作数 2\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 模型构建\n",
    "# ----------------------------------------- #\n",
    "\n",
    "agent = PPO(n_states=n_states,  # 状态数\n",
    "            n_hiddens=n_hiddens,  # 隐含层数\n",
    "            n_actions=n_actions,  # 动作数\n",
    "            actor_lr=actor_lr,  # 策略网络学习率\n",
    "            critic_lr=critic_lr,  # 价值网络学习率\n",
    "            lmbda = 0.95,  # 优势函数的缩放因子\n",
    "            epochs = 10,  # 一组序列训练的轮次\n",
    "            eps = 0.2,  # PPO中截断范围的参数\n",
    "            gamma=gamma,  # 折扣因子\n",
    "            device = device\n",
    "            )\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 训练--回合更新 on_policy\n",
    "# ----------------------------------------- #\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()[0]  # 环境重置\n",
    "    done = False  # 任务完成的标记\n",
    "    episode_return = 0  # 累计每回合的reward\n",
    "\n",
    "    # 构造数据集，保存每个回合的状态数据\n",
    "    transition_dict = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'next_states': [],\n",
    "        'rewards': [],\n",
    "        'dones': [],\n",
    "    }\n",
    "\n",
    "    while not done:\n",
    "        action = agent.take_action(state)  # 动作选择\n",
    "        next_state, reward, done, _, _  = env.step(action)  # 环境更新\n",
    "        # 保存每个时刻的状态\\动作\\...\n",
    "        transition_dict['states'].append(state)\n",
    "        transition_dict['actions'].append(action)\n",
    "        transition_dict['next_states'].append(next_state)\n",
    "        transition_dict['rewards'].append(reward)\n",
    "        transition_dict['dones'].append(done)\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        # 累计回合奖励\n",
    "        episode_return += reward\n",
    "\n",
    "    # 保存每个回合的return\n",
    "    return_list.append(episode_return)\n",
    "    # 模型训练\n",
    "    agent.learn(transition_dict)\n",
    "\n",
    "    # 打印回合信息\n",
    "    print(f'iter:{i}, return:{np.mean(return_list[-10:])}')\n",
    "\n",
    "# -------------------------------------- #\n",
    "# 绘图\n",
    "# -------------------------------------- #\n",
    "\n",
    "plt.plot(return_list)\n",
    "plt.title('return')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlseven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
